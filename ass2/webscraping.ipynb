{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Title    Price     Stock Rating  \\\n",
      "0                   A Light in the Attic  Â£51.77  In stock  Three   \n",
      "1                     Tipping the Velvet  Â£53.74  In stock    One   \n",
      "2                             Soumission  Â£50.10  In stock    One   \n",
      "3                          Sharp Objects  Â£47.82  In stock   Four   \n",
      "4  Sapiens: A Brief History of Humankind  Â£54.23  In stock   Five   \n",
      "\n",
      "      Description                                Product Information  \\\n",
      "0  No description  \\n\\nUPCa897fe39b1053632\\n\\n\\nProduct TypeBooks...   \n",
      "1  No description  \\n\\nUPC90fa61229261140a\\n\\n\\nProduct TypeBooks...   \n",
      "2  No description  \\n\\nUPC6957f44c3847a760\\n\\n\\nProduct TypeBooks...   \n",
      "3  No description  \\n\\nUPCe00eb4fd7b871a48\\n\\n\\nProduct TypeBooks...   \n",
      "4  No description  \\n\\nUPC4165285e1663650f\\n\\n\\nProduct TypeBooks...   \n",
      "\n",
      "                 Category  \n",
      "0              \\nPoetry\\n  \n",
      "1  \\nHistorical Fiction\\n  \n",
      "2             \\nFiction\\n  \n",
      "3             \\nMystery\\n  \n",
      "4             \\nHistory\\n  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL for the site, with a placeholder for the page number\n",
    "base_url = 'http://books.toscrape.com/catalogue/page-{}.html'\n",
    "\n",
    "# List to store book data\n",
    "books = []\n",
    "\n",
    "# Loop through the first 5 pages\n",
    "for page in range(1, 6):\n",
    "    # Request the page content\n",
    "    response = requests.get(base_url.format(page))\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all book entries on the page\n",
    "    for book in soup.select('article.product_pod'):\n",
    "        # Extract book details\n",
    "        title = book.h3.a['title']\n",
    "        price = book.select_one('p.price_color').text\n",
    "        stock = book.select_one('p.instock.availability').text.strip()\n",
    "        rating = book.p['class'][1]\n",
    "        description = book.select_one('p.subtitle').text if book.select_one('p.subtitle') else 'No description'\n",
    "        \n",
    "        # Get product details by requesting the book's detail page\n",
    "        product_url = book.h3.a['href']\n",
    "        product_response = requests.get(f\"http://books.toscrape.com/catalogue/{product_url}\")\n",
    "        product_soup = BeautifulSoup(product_response.text, 'html.parser')\n",
    "        \n",
    "        # Extract category and product information with error handling\n",
    "        category_element = product_soup.select_one('ul.breadcrumb li:nth-of-type(3)')\n",
    "        category = category_element.text if category_element else 'No category'\n",
    "\n",
    "        info_table = product_soup.select_one('table.table.table-striped')\n",
    "        info = info_table.text if info_table else 'No information available'\n",
    "        \n",
    "        # Append the book data to the list\n",
    "        books.append({\n",
    "            'Title': title,\n",
    "            'Price': price,\n",
    "            'Stock': stock,\n",
    "            'Rating': rating,\n",
    "            'Description': description,\n",
    "            'Product Information': info,\n",
    "            'Category': category\n",
    "        })\n",
    "\n",
    "# Convert the list of books to a DataFrame and save to CSV\n",
    "df_books = pd.DataFrame(books)\n",
    "df_books.to_csv('books_data.csv', index=False)\n",
    "print(df_books.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text           Author  \\\n",
      "0  “The world as we have created it is a process ...  Albert Einstein   \n",
      "1  “It is our choices, Harry, that show what we t...     J.K. Rowling   \n",
      "2  “There are only two ways to live your life. On...  Albert Einstein   \n",
      "3  “The person, be it gentleman or lady, who has ...      Jane Austen   \n",
      "4  “Imperfection is beauty, madness is genius and...   Marilyn Monroe   \n",
      "\n",
      "                                           Tags  \n",
      "0        change, deep-thoughts, thinking, world  \n",
      "1                            abilities, choices  \n",
      "2  inspirational, life, live, miracle, miracles  \n",
      "3              aliteracy, books, classic, humor  \n",
      "4                    be-yourself, inspirational  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Base URL for the quotes page\n",
    "base_url = 'http://quotes.toscrape.com'\n",
    "\n",
    "# List to store quote data\n",
    "quotes = []\n",
    "\n",
    "# Function to scrape a single page\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all quote entries on the page\n",
    "        for quote_div in soup.select('div.quote'):\n",
    "            text = quote_div.select_one('span.text').get_text(strip=True)\n",
    "            author = quote_div.select_one('small.author').get_text(strip=True)\n",
    "            tags = [tag.get_text(strip=True) for tag in quote_div.select('div.tags a.tag')]\n",
    "            \n",
    "            quotes.append({\n",
    "                'Text': text,\n",
    "                'Author': author,\n",
    "                'Tags': ', '.join(tags)\n",
    "            })\n",
    "    else:\n",
    "        print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n",
    "\n",
    "# Scrape the first few pages (modify as needed)\n",
    "for page in range(1, 6):  # Example: Scrape first 5 pages\n",
    "    page_url = f'{base_url}/page/{page}/'\n",
    "    scrape_page(page_url)\n",
    "\n",
    "# Convert the list of quotes to a DataFrame and save to CSV\n",
    "df_quotes = pd.DataFrame(quotes)\n",
    "df_quotes.to_csv('quotes_data.csv', index=False)\n",
    "print(df_quotes.head() if not df_quotes.empty else \"No data found to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Anwar Hossain (microbiologist)\n",
      "Content:\n",
      "\n",
      "Anwar Hossain is a Bangladeshi biologist and vice-chancellor of Jashore University of Science and Technology.[1][2] Hossain was a professor of the Department of Microbiology at the University of Dhaka.[3]\n",
      "\n",
      "\n",
      "Early life and education[edit]\n",
      "Hossain was born on 1 January 1958 in Satgharia, Louhajang Upazila, Munshiganj District, East Pakistan, Pakistan.[4] He did his bachelors and masters in Biochemistry and Molecular Biology at the University of Dhaka in 1981 and 1983 respectively.[5] He did his P...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Request a random Wikipedia page\n",
    "response = requests.get('https://en.wikipedia.org/wiki/Special:Random')\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extract the title and the first part of the content\n",
    "title = soup.find('h1', {'id': 'firstHeading'}).text\n",
    "content = soup.find('div', {'class': 'mw-parser-output'}).text\n",
    "\n",
    "# Print the title and a snippet of the content\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Content:\\n{content[:500]}...\")  # Print the first 500 characters of the content\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
